Training Loss,Test Loss
1.162605376180013,0.6106416320800782
0.936689316813151,0.4029789733886719
0.8795206507364909,0.3625807464599609
0.8527584350585937,0.34763548583984377
0.8380976587931315,0.34023084716796875
0.8289663391113281,0.3365427062988281
0.8210484085083007,0.3326484588623047
0.8157978251139323,0.32929284057617186
0.8105004247029622,0.32753204345703124
0.8062257873535156,0.32596796875
0.8033273274739583,0.3246232879638672
0.7997933263142903,0.3239524230957031
0.7964999694824219,0.32178297119140625
0.7942794998168945,0.3223139404296875
0.7919016932169597,0.3195934844970703
0.7898501118977864,0.31871664123535154
0.7877453847249349,0.31867440185546875
0.7860517400105794,0.31855121154785154
0.7842950561523437,0.3167589508056641
0.7825441335042318,0.3187665588378906
0.7813993174235027,0.31640732116699216
0.779416650390625,0.31549747314453125
0.7785331207275391,0.3166157409667969
0.776896267191569,0.31483996887207033
0.7758289952596029,0.3165413360595703
0.7745225056966146,0.3162616760253906
0.7735010950724284,0.3148385498046875
0.7676213724772135,0.3137850372314453
0.7670430150349935,0.3116433074951172
0.7668351531982421,0.3115531311035156
0.7666353012084961,0.31150089721679686
0.7663113367716471,0.3115218994140625
0.766161870320638,0.31139187622070313
0.7660116470336914,0.3113321319580078
0.7657812845865886,0.3114082275390625
0.7656460815429688,0.3111600280761719
0.7653466028849284,0.31101949768066406
0.7653057408650716,0.31117400817871094
0.7651464401245117,0.3109141387939453
0.764799809773763,0.31111302795410156
0.7648010426839192,0.3110306701660156
0.7644586029052735,0.31100000610351564
0.7640818008422852,0.3108524932861328
0.7640201466878255,0.31084266662597654
0.7640580942789713,0.31080631408691406
0.7639621551513672,0.3107775848388672
0.7639093399047852,0.3107972900390625
0.7639182403564453,0.31088888244628904
0.7639171066284179,0.3108256164550781
0.7638155364990235,0.310725146484375
